2017-05-11-CS350

Prof Email: lanortha@uwaterloo.ca


#### twothreads problem

* Suppose there are two threads in a system
* Round robin scheduling
* Scheduling quantum: Q ms
* Single processor

```
for i from 1 to N do
    compute for C ms
    sleep for S ms
end
```

Note: To solve these thread problems, try to envision the flow.

1. First, assume that C < S and C < Q. Suppose both threads start at time `t=0`. At what time will both threads be finished?
    * Draw the cycle
    ```
    XXXYYYY|XXXYYYY| ... |XXXYYYY|
       XXXY|YYYXXXY| ... |   XXXY|YYY
    ```
        * Here, we have `(C+S)N + S`, with S extra ms at the end
2. Second, assume that S < C < quantum
    * Again, the quantum has no effect
    ```
    XXXXXX|YYYZZZ|XXXXXX|YYYZZZ| ... |YYY   |
          |XXXXXX|YYYZZZ|XXXXXX| ... |XXXXXX|YYY
    ```
    * X = computer, Y = sleep, Z = extra wait
        * Here, we have `2NC + S`


#### Two-Thread Example (P1) [slide 27]
...



### Synchronization


#### Key concepts

Critical sections, mutual exclusion, test-and-set, spinlocks, blocking and blocking locks, semaphores, condition variables, deadlocks

We use multi-threading to actually have threads work together to achieve some goal. Since all threads share the address space and global data (everything but the stack), we need to make sure these threads don't fuck up the shared varialbes by using it at the wrong time.

* Suppose `int x = 0;`
    - How do we have multiple threads reading and writing the same variable at the same time? What will happen?
        + What if thread2 uses `x` while it is in the middle of being changed?
        + How can we know thread2 is using the right `x` value?
- If one thread calls add() on a `x` and another calls sub() on `x` at the same time, we couldn't possily know what the result will be.
- To analyze this, we need to understand what operations like `x++` and `x--` actually do
    - In MIPS, when we do x++, we have at least 3 operations. Load the value. Update it. Save the value
        + **This can be interrupted at any point** by another thread using `x`
* **Critical Section Examples [slides 5,6,7]**
    - We have no control over where, exactly, thread1's MIPs operation is interrupted.
    - We know our context switches saves the register values without problem.
    - Sometimes, thread2's operations are pointless because thread1 hadn't had a chance to save its computed value yet
    - Note that no two operations can use the same address at the same time. Inevitably, one thread's operation will run before the other.

When the result depends on the exact order in which our methods are called, we have a **race condition**

* Not really what we want. When we do add() and sub() we know exactly what the result should be. However, we can't ever be sure.

**Solution**
Force certain sections of the code to execute in a particular order. By doing this, we avoid having multiple threads trying to read/write to the same variable at the same time.


#### Volatile

If a variable is *volatile*, the compiler will be forced to load and store the value on every use.

* Recall in assembly optimization, sometimes we take a shortcut and avoid calling `lw` and `sw` when we don't really need to.
    - Sometimes we store commonly used variables in a register
    - Other times, we might realize that we call `lw` even though we did that from another operation

**Another Critical Section Example [slide 9]**

1. `list *lp` has one item
2. T1 and T2 call `list_remove_front`
3. Suppose T1 sees that `lp` is not empty. It reaches `element=lp->first` but doesn't call it (yet).
4. We have a context switch to T2
5. T2 finishes `list_remove_front`. It sets `num_in_list` to 0, and sets `list->first=NULL`.
6. T1 starts running again. But now, `list->front` is NULL. Segmentation fault
    
* If two threads call `list_remove_front`, one of them should remove the front, and the other should **not** crash. It should just do nothing.
* This is not the only race condition (not even close)

**More Critical Section Example [slide 10]**

We are appending to the end of the list.

1. Suppose our list is empty
2. T1 and T2 call `list_append`
3. T1 sets `element->item = new_item (== 1)` and the assertion succeeds.
4. T1 checks `is_empty(lp)`, which is true, but doesn't run the condition block (yet)
4. T2 runs. It sets `element-> = new_item (== 2)`
5. T2 adds the new element and increments `lp->num_in_list`
6. T1 runs, again. However, it's inside the `is_empty(lp)` block. After T2, `lp` is not empty.
7. T1 sets `lp->first = element`. This element was created by T1.
8. Problem: T2 created an element using malloc, but it was overwritten by T1.
    * This is a memory leak
    * Moreover, it's like T2 didn't do anything to the array

Note: when using dynamic array in assignments, pay attention to this


#### Mutual Exclusion

To prevent race conditions, we can enforce mutual exclusion on critical sections in the code

* One, and only one thread is in these critical section(s) at a time
    - example: only one person in a bathroom stall at a time. The stall is a critical section
        + If you're the one that locked the bathroom stall door, you should be the only one that can unlock the door.
        + If someone else can unlock the door from the outside, we still have a race condition
* How do we have a locking door around our critical section?
    - Simplest version: a global variable `lock=true`. If it's false, we can't use the section
        + Before we use the critical section, we `Acquire(&lock)`
        + After, we `Release(&lock)`

**Example**

```
bool v, lock=false;

Acquire(bool *lock) {
    while(*lock) {}; // knock on the door until it opens up. When the lock is finally false, the door opens

    *lock=true;
}
```

* Does this work? No. Why? Look ahead to how we can fix it